{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samirbehindscreen-jpg/DTSC3020/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3b608d-25b4-4eab-82a8-6b7a14fbe3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3e450e-a983-46c0-df00-305dc41c46bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b09248-688a-419b-e98b-abcbd6243b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   Country Alpha-2 Alpha-3  Numeric\n",
            "                                                    Zambia      ZM     ZMB      894\n",
            "                                                     Yemen      YE     YEM      887\n",
            "                                                     Samoa      WS     WSM      882\n",
            "                                         Wallis and Futuna      WF     WLF      876\n",
            "                        Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "                                                Uzbekistan      UZ     UZB      860\n",
            "                                                   Uruguay      UY     URY      858\n",
            "                                              Burkina Faso      BF     BFA      854\n",
            "                                     Virgin Islands (U.S.)      VI     VIR      850\n",
            "                            United States of America (the)      US     USA      840\n",
            "                              Tanzania, United Republic of      TZ     TZA      834\n",
            "                                               Isle of Man      IM     IMN      833\n",
            "                                                    Jersey      JE     JEY      832\n",
            "                                                  Guernsey      GG     GGY      831\n",
            "United Kingdom of Great Britain and Northern Ireland (the)      GB     GBR      826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-167905565.py:11: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)  # requires lxml installed\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a candidate country-codes table from the HTML.\n",
        "    Strategy:\n",
        "      1) read all tables with pandas.read_html\n",
        "      2) prefer the one that already contains Country/Alpha-2/Alpha-3/Numeric (case-insensitive)\n",
        "      3) otherwise, pick the widest table that has at least 3 columns\n",
        "      4) flatten headers before returning\n",
        "    \"\"\"\n",
        "    tables = pd.read_html(html)  # requires lxml installed\n",
        "    if not tables:\n",
        "        raise ValueError(\"No tables were found in the provided HTML.\")\n",
        "\n",
        "    required = {\"country\", \"alpha-2\", \"alpha-3\", \"numeric\"}\n",
        "    chosen = None\n",
        "\n",
        "    for t in tables:\n",
        "        if t.shape[1] >= 3:\n",
        "            cols_norm = {str(c).strip().lower() for c in t.columns}\n",
        "            if required.issubset(cols_norm):\n",
        "                chosen = t\n",
        "                break\n",
        "\n",
        "    if chosen is None:\n",
        "        # Fallback: pick the widest table with >= 3 columns\n",
        "        candidates = [t for t in tables if t.shape[1] >= 3]\n",
        "        if not candidates:\n",
        "            raise ValueError(\"No table with >= 3 columns found.\")\n",
        "        chosen = max(candidates, key=lambda d: d.shape[1])\n",
        "\n",
        "    return flatten_headers(chosen.copy())\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean columns: strip, standardize header names, uppercase Alpha-2/Alpha-3,\n",
        "    cast Numeric to pandas nullable Int64, and drop clearly invalid rows.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) Normalize column names (trim)\n",
        "    df.columns = [str(c).strip() for c in df.columns]\n",
        "\n",
        "    # 2) Map common variants to canonical names\n",
        "    colmap = {}\n",
        "    for c in df.columns:\n",
        "        cl = c.lower()\n",
        "        if cl == \"country\":\n",
        "            colmap[c] = \"Country\"\n",
        "        elif cl in (\"alpha-2\", \"alpha-2 code\", \"alpha2\", \"alpha 2\"):\n",
        "            colmap[c] = \"Alpha-2\"\n",
        "        elif cl in (\"alpha-3\", \"alpha-3 code\", \"alpha3\", \"alpha 3\"):\n",
        "            colmap[c] = \"Alpha-3\"\n",
        "        elif cl in (\"numeric\", \"numeric code\", \"num\", \"numericcode\"):\n",
        "            colmap[c] = \"Numeric\"\n",
        "    df = df.rename(columns=colmap)\n",
        "\n",
        "    # 3) Keep required cols first (others can remain after them)\n",
        "    required = [\"Country\", \"Alpha-2\", \"Alpha-3\", \"Numeric\"]\n",
        "    ordered_cols = [c for c in required if c in df.columns] + [c for c in df.columns if c not in required]\n",
        "    df = df[ordered_cols]\n",
        "\n",
        "    # 4) Trim whitespace in string columns\n",
        "    for c in df.select_dtypes(include=[\"object\"]).columns:\n",
        "        df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "    # 5) Uppercase Alpha codes (tolerate missing)\n",
        "    for c in (\"Alpha-2\", \"Alpha-3\"):\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].where(df[c].notna(), None)\n",
        "            df[c] = df[c].astype(str).str.upper().replace({\"NAN\": pd.NA})\n",
        "\n",
        "    # 6) Numeric → digits only, then nullable integer\n",
        "    if \"Numeric\" in df.columns:\n",
        "        numeric_clean = df[\"Numeric\"].astype(str).str.extract(r\"(\\d+)\", expand=False)\n",
        "        df[\"Numeric\"] = pd.to_numeric(numeric_clean, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    # 7) Drop rows that are clearly invalid (e.g., missing Country or header echoes)\n",
        "    if \"Country\" in df.columns:\n",
        "        df = df[~df[\"Country\"].astype(str).str.fullmatch(r\"\\s*Country\\s*\", case=False, na=False)]\n",
        "        df = df[df[\"Country\"].notna() & (df[\"Country\"].astype(str).str.len() > 0)]\n",
        "\n",
        "    # Optional: drop exact duplicates\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Sort descending by Numeric (dropping NA for the sort) and return Top-N.\n",
        "    \"\"\"\n",
        "    if \"Numeric\" not in df.columns:\n",
        "        raise KeyError(\"Column 'Numeric' is missing; cannot sort.\")\n",
        "    out = df.dropna(subset=[\"Numeric\"]).sort_values(\"Numeric\", ascending=False, kind=\"mergesort\")\n",
        "    return out.head(top).reset_index(drop=True)\n",
        "URL = \"https://www.iban.com/country-codes\"\n",
        "\n",
        "html = fetch_html(URL)\n",
        "raw = q1_read_table(html)\n",
        "clean = q1_clean(raw)\n",
        "\n",
        "# Save CSV\n",
        "clean.to_csv(\"data_q1.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Print Top-15 by Numeric (desc)\n",
        "top15 = q1_sort_top(clean, 15)\n",
        "print(top15.to_string(index=False))\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urljoin\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd   # <— make sure this import is present too\n",
        "\n",
        "HN_BASE = \"https://news.ycombinator.com/\"\n",
        "\n",
        "def _parse_int(text: str) -> int:\n",
        "    \"\"\"Return the first integer found in text; if none, 0.\"\"\"\n",
        "    if text is None:\n",
        "        return 0\n",
        "    m = re.search(r\"(\\d+)\", str(text))\n",
        "    return int(m.group(1)) if m else 0\n",
        "\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    items = []\n",
        "\n",
        "    for row in soup.select(\"tr.athing\"):\n",
        "        rank_txt = row.select_one(\"span.rank\")\n",
        "        rank = _parse_int(rank_txt.get_text(strip=True) if rank_txt else None)\n",
        "\n",
        "        a = row.select_one(\".titleline a\") or row.select_one(\"a.storylink\")\n",
        "        title = a.get_text(strip=True) if a else \"\"\n",
        "        href = a.get(\"href\", \"\") if a else \"\"\n",
        "        link = urljoin(HN_BASE, href) if href else \"\"\n",
        "\n",
        "        sub = row.find_next_sibling(\"tr\")\n",
        "        subtext = sub.select_one(\".subtext\") if sub else None\n",
        "\n",
        "        # Safely get points_txt, handling cases where 'span.score' might not be found\n",
        "        points_element = subtext.select_one(\"span.score\") if subtext else None\n",
        "        points_txt = points_element.get_text(strip=True) if points_element else None\n",
        "        points = _parse_int(points_txt)\n",
        "\n",
        "        u = subtext.select_one(\"a.hnuser\") if subtext else None\n",
        "        user = u.get_text(strip=True) if u else \"\"\n",
        "\n",
        "        comments = 0\n",
        "        if subtext:\n",
        "            for a2 in reversed(subtext.select(\"a\")):\n",
        "                t = a2.get_text(strip=True).lower()\n",
        "                if \"comment\" in t or \"discuss\" in t:\n",
        "                    comments = _parse_int(t)\n",
        "                    break\n",
        "\n",
        "        items.append({\n",
        "            \"rank\": rank,\n",
        "            \"title\": title or \"\",\n",
        "            \"link\": link or \"\",\n",
        "            \"points\": points,\n",
        "            \"comments\": comments,\n",
        "            \"user\": user or \"\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(items, columns=[\"rank\", \"title\", \"link\", \"points\", \"comments\", \"user\"])\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for c in (\"title\", \"link\", \"user\"):\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(\"\").astype(str).str.strip()\n",
        "\n",
        "    for c in (\"rank\", \"points\", \"comments\"):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    if \"title\" in df.columns and \"link\" in df.columns:\n",
        "        df = df[(df[\"title\"] != \"\") | (df[\"link\"] != \"\")]\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    if \"points\" not in df.columns:\n",
        "        raise KeyError(\"Column 'points' missing; cannot sort.\")\n",
        "    return df.sort_values(\"points\", ascending=False, kind=\"mergesort\").head(top).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# === Run ===\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "html = fetch_html(URL)\n",
        "raw = q2_parse_items(html)\n",
        "clean = q2_clean(raw)\n",
        "\n",
        "clean.to_csv(\"data_q2.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "top15 = q2_sort_top(clean, 15)\n",
        "print(top15.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnWU3FiIRg4u",
        "outputId": "c3740faf-cade-4c16-8aa0-6e59c30b4069"
      },
      "id": "CnWU3FiIRg4u",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " rank                                                                         title                                                                                                   link  points  comments         user\n",
            "    9                                                      Leaving Meta and PyTorch                                    https://soumith.ch/blog/2025-11-06-leaving-meta-and-pytorch.md.html     652       158     saikatsg\n",
            "   30                            Meta projected 10% of 2024 revenue came from scams https://sherwood.news/tech/meta-projected-10-of-2024-revenue-came-from-scams-and-banned-goods-reuters/     565       438      donohoe\n",
            "   11                                                               A Fond Farewell                                      https://www.farmersalmanac.com/fond-farewell-from-farmers-almanac     543       196       erhuve\n",
            "   23       Rockstar employee shares account of the company's union-busting efforts                              https://gtaforums.com/topic/1004182-rockstar-games-alleged-union-busting/     379       236       mrzool\n",
            "   15 Denmark's government aims to ban access to social media for children under 15          https://apnews.com/article/denmark-social-media-ban-children-7862d2a8cc590b4969c8931a01adc7f4     294       211         c420\n",
            "    7                                                                  I Love OCaml                                                        https://mccd.space/posts/ocaml-the-worlds-best/     250       157        art-w\n",
            "   27               OpenMW 0.50.0 Released – open-source Morrowind reimplementation                                                        https://openmw.org/2025/openmw-0-50-0-released/     239        87     agluszak\n",
            "   29                                            We chose OCaml to write Stategraph                                                         https://stategraph.dev/blog/why-we-chose-ocaml     135       100    lawnchair\n",
            "    8                                                         James Watson has died                                      https://www.nytimes.com/2025/11/07/science/james-watson-dead.html     128        30    granzymes\n",
            "   20                                                                PyTorch Helion                                                                       https://pytorch.org/blog/helion/     122        36       jarbus\n",
            "    1      Myna: Monospace typeface designed for symbol-heavy programming languages                                                                 https://github.com/sayyadirfanali/Myna     103        43  birdculture\n",
            "   18                                       Apple is crossing a Steve Jobs red line                              https://kensegall.com/2025/11/07/apple-is-crossing-a-steve-jobs-red-line/      97        77          zdw\n",
            "    2                                                        Ruby Solved My Problem                                      https://newsletter.masilotti.com/p/ruby-already-solved-my-problem      93        22 joemasilotti\n",
            "   24 Toxic Salton Sea dust triggers changes in lung microbiome after just one week                                      https://phys.org/news/2025-10-toxic-salton-sea-triggers-lung.html      84        29    PaulHoule\n",
            "   17                              My Experience of building Bytebeat player in Zig                                                            https://blog.karanjanthe.me/posts/zig-beat/      72         8      KMJ-007\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}